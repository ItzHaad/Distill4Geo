{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pb6_wSDwPo5r"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.17 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from lightning import LightningModule, Trainer, LightningDataModule\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from fastervit import create_model\n",
        "import timm\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "from distill_transforms import get_transforms_val\n",
        "from torch.cuda.amp import autocast\n",
        "DIM = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1YGyMj_ugBDC"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('CVUSA/val.csv')\n",
        "val_image_pairs_details = df.values.tolist()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "prob_rotate: float = 0.75          # rotates the sat image and ground images simultaneously\n",
        "prob_flip: float = 0.5             # flipping the sat image and ground images simultaneously\n",
        "student_sat_size: tuple = (224, 224)\n",
        "student_street_size: tuple = (224, 224)\n",
        "\n",
        "sat_transforms_val_student, street_transforms_val_student = get_transforms_val(student_sat_size, student_street_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k8yh-l3QgiDC"
      },
      "outputs": [],
      "source": [
        "class WeightDistillEvalSet(Dataset):\n",
        "    def __init__(self, image_pairs_details):\n",
        "        self.image_pairs_details = image_pairs_details\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs_details)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sat_image_path = 'CVUSA/'+ self.image_pairs_details[idx][0]\n",
        "        street_image_path = 'CVUSA/'+ self.image_pairs_details[idx][1]\n",
        "\n",
        "        # Open and process the satellite image\n",
        "        with Image.open(sat_image_path) as sat_image:\n",
        "            sat_img = sat_transforms_val_student(image= np.asarray(sat_image))['image']\n",
        "        # Open and process the street image\n",
        "        with Image.open(street_image_path) as street_image:\n",
        "            street_img = street_transforms_val_student(image= np.asarray(street_image))['image']\n",
        "        \n",
        "        return sat_img, street_img\n",
        "\n",
        "# Usage\n",
        "val_testset = WeightDistillEvalSet(val_image_pairs_details)\n",
        "\n",
        "val_testloader = torch.utils.data.DataLoader(val_testset, batch_size= 32, num_workers = 6, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def contrastive_loss(logits, dim):\n",
        "    neg_ce = torch.diag(F.log_softmax(logits, dim=dim))\n",
        "    return -neg_ce.sum()\n",
        "\n",
        "def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n",
        "    caption_loss = contrastive_loss(similarity, dim=0)\n",
        "    image_loss = contrastive_loss(similarity, dim=1)\n",
        "    return (caption_loss + image_loss) / 2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContrastiveModel(LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.strict_loading = False\n",
        "        # self.teacher_model = TimmModel('convnext_base.fb_in22k_ft_in1k_384', pretrained=True, img_size= 384)\n",
        "        # self.teacher_model.load_state_dict(torch.load(\"pretrained/cvusa/convnext_base.fb_in22k_ft_in1k_384/weights_e40_98.6830.pth\", weights_only=True))\n",
        "        # self.teacher_model.eval()\n",
        "        # for param in self.teacher_model.parameters():\n",
        "        #     param.requires_grad = False\n",
        "            \n",
        "        self.sat_model = create_model('faster_vit_0_224', pretrained=True, model_path=\"/tmp/faster_vit_0.pth.tar\")\n",
        "        self.sat_linear = nn.Linear(1000, 1024)\n",
        "        \n",
        "        self.street_model = create_model('faster_vit_0_224', pretrained=True, model_path=\"/tmp/faster_vit_0.pth.tar\")\n",
        "        self.street_linear = nn.Linear(1000, 1024)\n",
        "        \n",
        "        # self.logit_scale = nn.Parameter(torch.tensor(np.log(1 / 0.07), dtype=torch.float32))\n",
        "        \n",
        "        # self.bias = nn.Parameter(torch.tensor(-10, dtype=torch.float32))\n",
        "        \n",
        "        self.loss = nn.CosineEmbeddingLoss()\n",
        "                \n",
        "    def forward(self, teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat):\n",
        "        student_sat_features = self.sat_linear(self.sat_model(stud_sat_feat))\n",
        "        student_street_features = self.street_linear(self.street_model(stud_street_feat))\n",
        "        \n",
        "        with autocast():\n",
        "            teacher_sat_features = self.teacher_model(teacher_sat_feat)\n",
        "            teacher_street_features = self.teacher_model(teacher_street_feat)\n",
        "            \n",
        "        # loss_sat = self.loss(student_sat_features, teacher_sat_features)\n",
        "        # loss_street = self.loss(student_street_features, teacher_street_features)\n",
        "        \n",
        "        loss_sat = self.loss(student_sat_features, teacher_sat_features, torch.ones(student_sat_features.shape[0], device= self.device))\n",
        "        loss_street = self.loss(student_street_features, teacher_street_features, torch.ones(student_street_features.shape[0], device= self.device))\n",
        "        \n",
        "        # normalized features\n",
        "        # student_sat_features = F.normalize(student_sat_features, dim=-1)\n",
        "        # student_street_features = F.normalize(student_street_features, dim=-1)\n",
        "        \n",
        "        # teacher_sat_features = F.normalize(teacher_sat_features, dim=-1)\n",
        "        # teacher_street_features = F.normalize(teacher_street_features, dim=-1)\n",
        "\n",
        "        # # cosine similarity as logits\n",
        "        # logit_scale =  self.logit_scale.exp()\n",
        "\n",
        "        # student_logits = logit_scale * student_sat_features @ student_street_features.t() + self.bias\n",
        "        # teacher_logits = teacher_sat_features @ teacher_street_features.t()\n",
        "        \n",
        "        # n = student_logits.size(0)\n",
        "\t\n",
        "        # # -1 for off-diagonals and 1 for diagonals\n",
        "        # labels = 2 * teacher_logits - 1\n",
        "        \n",
        "        # # pairwise sigmoid loss\n",
        "        # c_loss = -torch.sum(F.logsigmoid(labels * student_logits)) / n\n",
        "        \n",
        "        return (loss_sat + loss_street) / 2\n",
        "\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat = batch\n",
        "        loss = self(teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat)\n",
        "    \n",
        "        #n = logits.size(0)\n",
        "\t\n",
        "        # -1 for off-diagonals and 1 for diagonals\n",
        "        #labels = 2 * torch.eye(n, device=logits.device) - 1\n",
        "        \n",
        "        # pairwise sigmoid loss\n",
        "        #loss= -torch.sum(F.logsigmoid(labels * logits)) / n\n",
        "\n",
        "        # alpha = self.current_epoch / self.trainer.max_epochs\n",
        "        \n",
        "        # loss = (1 - alpha) * mse_loss + alpha * c_loss\n",
        "        \n",
        "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat = batch\n",
        "        loss = self(teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat)\n",
        "\n",
        "        #n = logits.size(0)\n",
        "\t\n",
        "        # # -1 for off-diagonals and 1 for diagonals\n",
        "        #labels = 2 * torch.eye(n, device=logits.device) - 1\n",
        "        \n",
        "        # # pairwise sigmoid loss\n",
        "        #loss= -torch.sum(F.logsigmoid(labels * logits)) / n\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "            \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr= 1e-4)\n",
        "        scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs = 1, max_epochs = 512)\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}] #optimizer\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        # Exclude teacher model's weights from checkpoint\n",
        "        checkpoint['state_dict'] = {k: v for k, v in checkpoint['state_dict'].items() if 'teacher_model' not in k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ContrastiveModel(\n",
              "  (sat_model): FasterViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Identity()\n",
              "      (conv_down): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (4): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (levels): ModuleList(\n",
              "      (0): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): ConvBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): ConvBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.013)\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.027)\n",
              "          )\n",
              "          (1): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.040)\n",
              "          )\n",
              "          (2): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.053)\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.067)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.067)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (1): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.080)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.080)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (2): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.093)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.093)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (3): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.107)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.107)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (4): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.120)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.120)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (5): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.133)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.133)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "        (global_tokenizer): TokenInitializer(\n",
              "          (pos_embed): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "          (to_global_feature): Sequential(\n",
              "            (pos): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.147)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.160)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.173)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.187)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.200)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "    (head): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  )\n",
              "  (sat_linear): Linear(in_features=1000, out_features=1024, bias=True)\n",
              "  (street_model): FasterViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Identity()\n",
              "      (conv_down): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (4): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (levels): ModuleList(\n",
              "      (0): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): ConvBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): ConvBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.013)\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.027)\n",
              "          )\n",
              "          (1): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.040)\n",
              "          )\n",
              "          (2): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.053)\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.067)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.067)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (1): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.080)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.080)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (2): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.093)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.093)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (3): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.107)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.107)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (4): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.120)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.120)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (5): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.133)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.133)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "        (global_tokenizer): TokenInitializer(\n",
              "          (pos_embed): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "          (to_global_feature): Sequential(\n",
              "            (pos): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.147)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.160)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.173)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.187)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.200)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "    (head): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  )\n",
              "  (street_linear): Linear(in_features=1000, out_features=1024, bias=True)\n",
              "  (loss): CosineEmbeddingLoss()\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "distill_model = ContrastiveModel.load_from_checkpoint(\"./logs/KD_EMBEDDING_CD_ONLY_CVUSA/version_0/checkpoints/tinyvit.ckpt\")\n",
        "distill_model = distill_model.to(device)\n",
        "distill_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = TimmModel('convnext_base.fb_in22k_ft_in1k_384', pretrained=True, img_size= 384)\n",
        "# model.load_state_dict(torch.load(\"pretrained/cvusa/convnext_base.fb_in22k_ft_in1k_384/weights_e40_98.6830.pth\", weights_only=True))\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = model.to(device)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Scr3JOak-Are",
        "outputId": "e86d566c-f58c-4c86-8115-4f35872c665a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 278/278 [00:22<00:00, 12.55it/s]\n"
          ]
        }
      ],
      "source": [
        "all_street_embed = []\n",
        "all_sat_embed = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_data in tqdm(val_testloader):\n",
        "        sat_img, street_img = batch_data\n",
        "        \n",
        "        # Street Image Embeddings\n",
        "        street_features = distill_model.street_linear(distill_model.street_model(street_img.to(device)))\n",
        "        all_street_embed.append(street_features.detach().cpu())\n",
        "\n",
        "        # Original Satellite Image Embeddings\n",
        "        sat_features = distill_model.sat_linear(distill_model.sat_model(sat_img.to(device)))\n",
        "        all_sat_embed.append(sat_features.detach().cpu())\n",
        "        \n",
        "\n",
        "    # Save the embeddings\n",
        "    joblib.dump(torch.vstack(all_street_embed), 'MB1/cvusa_street_embed.joblib')\n",
        "    joblib.dump(torch.vstack(all_sat_embed), 'MB1/cvusa_sat_embed.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "cvusa_sat_features = joblib.load(open(\"MB1/cvusa_sat_embed.joblib\", 'rb'))\n",
        "cvusa_street_features = joblib.load(open(\"MB1/cvusa_street_embed.joblib\", 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8883, 1024])\n",
            "torch.Size([8883, 1024])\n"
          ]
        }
      ],
      "source": [
        "print(cvusa_sat_features.shape)\n",
        "print(cvusa_street_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "cvusa_sat_features = F.normalize(cvusa_sat_features, dim=-1)\n",
        "cvusa_street_features = F.normalize(cvusa_street_features, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = faiss.StandardGpuResources()\n",
        "\n",
        "cvusa_index = faiss.IndexFlatIP(DIM)\n",
        "cvusa_index = faiss.index_cpu_to_gpu(res, 1, cvusa_index)\n",
        "\n",
        "cvusa_index.add(cvusa_sat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "cvusa_top_1_per = int(0.01*len(cvusa_street_features))\n",
        "\n",
        "_, cvusa_ids = cvusa_index.search(x=cvusa_street_features, k=cvusa_top_1_per)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For CVUSA, Top 1 Accuracy: 97.43329956095914%\n",
            "For CVUSA, Top 5 Accuracy: 99.58347405155916%\n",
            "For CVUSA, Top 10 Accuracy: 99.76359338061465%\n",
            "For CVUSA, Top 50 Accuracy: 99.85365304514241%\n",
            "For CVUSA, Top 100 Accuracy: 99.86491050320838%\n",
            "For CVUSA, Top 88 Accuracy: 99.86491050320838%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cvusa_matches = {1: 0, 5: 0, 10: 0, 50: 0, 100: 0, cvusa_top_1_per: 0}\n",
        "\n",
        "for i in range(len(cvusa_street_features)):\n",
        "    if i == cvusa_ids[i][0]:\n",
        "        for key in list(cvusa_matches.keys()):\n",
        "            cvusa_matches[key]+=1  \n",
        "    elif i in cvusa_ids[i][0:5]:\n",
        "        for key in list(cvusa_matches.keys())[1:]:\n",
        "            cvusa_matches[key]+=1 \n",
        "    elif i in cvusa_ids[i][0:10]:\n",
        "        for key in list(cvusa_matches.keys())[2:]:\n",
        "            cvusa_matches[key]+=1\n",
        "    elif i in cvusa_ids[i][0:50]:\n",
        "        for key in list(cvusa_matches.keys())[3:]:\n",
        "            cvusa_matches[key]+=1\n",
        "    elif i in cvusa_ids[i][0:100]:\n",
        "        for key in list(cvusa_matches.keys())[4:]:\n",
        "            cvusa_matches[key]+=1\n",
        "    elif i in cvusa_ids[i][0:cvusa_top_1_per]:\n",
        "        for key in list(cvusa_matches.keys())[5:]:\n",
        "            cvusa_matches[key]+=1\n",
        "\n",
        "for keys in cvusa_matches.keys():\n",
        "    cvusa_matches[keys] = (cvusa_matches[keys] * 100) / len(cvusa_street_features)\n",
        "    print(f\"For CVUSA, Top {keys} Accuracy: {cvusa_matches[keys]}%\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def calculate_scores(query_features, reference_features, step_size=500, ranks=[1,5,10]):\n",
        "\n",
        "    topk = copy.deepcopy(ranks)\n",
        "    Q = len(query_features)\n",
        "    R = len(reference_features)\n",
        "    \n",
        "    steps = Q // step_size + 1\n",
        "    \n",
        "    similarity = []\n",
        "    \n",
        "    for i in range(steps):\n",
        "        \n",
        "        start = step_size * i\n",
        "        \n",
        "        end = start + step_size\n",
        "          \n",
        "        sim_tmp = query_features[start:end] @ reference_features.T\n",
        "        \n",
        "        similarity.append(sim_tmp)\n",
        "     \n",
        "    # matrix Q x R\n",
        "    similarity = torch.cat(similarity, dim=0)\n",
        "    \n",
        "\n",
        "    topk.append(R//100)\n",
        "    \n",
        "    results = np.zeros([len(topk)])\n",
        "    \n",
        "    \n",
        "    bar = tqdm(range(Q))\n",
        "    \n",
        "    for i in bar:\n",
        "        \n",
        "        # similiarity value of gt reference\n",
        "        gt_sim = similarity[i, i]\n",
        "        \n",
        "        # number of references with higher similiarity as gt\n",
        "        higher_sim = similarity[i,:] > gt_sim\n",
        "        \n",
        "         \n",
        "        ranking = higher_sim.sum()\n",
        "        for j, k in enumerate(topk):\n",
        "            if ranking < k:\n",
        "                results[j] += 1.\n",
        "                        \n",
        "        \n",
        "    results = results/ Q * 100.\n",
        " \n",
        "    \n",
        "    bar.close()\n",
        "    \n",
        "    # wait to close pbar\n",
        "    time.sleep(0.1)\n",
        "    \n",
        "    string = []\n",
        "    for i in range(len(topk)-1):\n",
        "        \n",
        "        string.append('Recall@{}: {:.4f}'.format(topk[i], results[i]))\n",
        "        \n",
        "    string.append('Recall@top1: {:.4f}'.format(results[-1]))            \n",
        "        \n",
        "    print(' - '.join(string)) \n",
        "\n",
        "    return results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8883/8883 [00:00<00:00, 24298.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@1: 97.9174 - Recall@5: 99.5835 - Recall@10: 99.7636 - Recall@top1: 99.8649\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "97.9173702577958"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_scores(cvusa_street_features, cvusa_sat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(dist_array, top_k):\n",
        "    accuracy = 0.0\n",
        "    data_amount = 0.0\n",
        "    for i in range(dist_array.shape[0]):\n",
        "        gt_dist = dist_array[i,i]\n",
        "        prediction = torch.sum(dist_array[:, i] < gt_dist) \n",
        "        if prediction < top_k:\n",
        "            accuracy += 1.0\n",
        "        data_amount += 1.0\n",
        "    accuracy /= data_amount\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   compute accuracy\n",
            "start\n",
            "top1 : 0.9800742992232354\n",
            "top5 : 0.9954970167736125\n",
            "top10 : 0.9972982100641675\n",
            "top1% : 0.9986491050320837\n"
          ]
        }
      ],
      "source": [
        "print('   compute accuracy')\n",
        "dist_array = 2.0 - 2.0 * torch.matmul(cvusa_street_features, cvusa_sat_features.T)\n",
        "\n",
        "top1_percent = int(dist_array.shape[0] * 0.01) + 1\n",
        "val_accuracy = torch.zeros((1, top1_percent)).cuda()\n",
        "\n",
        "print('start')\n",
        "\n",
        "print('top1', ':', validate(dist_array, 1))\n",
        "print('top5', ':', validate(dist_array, 5))\n",
        "print('top10', ':', validate(dist_array, 10))\n",
        "print('top1%', ':', validate(dist_array, top1_percent))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
