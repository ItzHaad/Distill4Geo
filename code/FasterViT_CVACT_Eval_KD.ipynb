{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pb6_wSDwPo5r"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n",
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.18 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from lightning import LightningModule, Trainer, LightningDataModule\n",
        "from collections import OrderedDict\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from fastervit import create_model\n",
        "import timm\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "from distill_transforms import get_transforms_val\n",
        "DIM = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1YGyMj_ugBDC"
      },
      "outputs": [],
      "source": [
        "val_df = pd.read_csv('CVACT/val.csv')\n",
        "test_df = pd.read_csv('CVACT/test.csv')\n",
        "\n",
        "val_image_pairs_details = val_df.values.tolist()\n",
        "test_image_pairs_details = test_df.values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "prob_rotate: float = 0.75          # rotates the sat image and ground images simultaneously\n",
        "prob_flip: float = 0.5             # flipping the sat image and ground images simultaneously\n",
        "student_sat_size: tuple = (224, 224)\n",
        "student_street_size: tuple = (224, 224)\n",
        "\n",
        "sat_transforms_val_student, street_transforms_val_student = get_transforms_val(student_sat_size, student_street_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k8yh-l3QgiDC"
      },
      "outputs": [],
      "source": [
        "class WeightDistillEvalSet(Dataset):\n",
        "    def __init__(self, image_pairs_details):\n",
        "        self.image_pairs_details = image_pairs_details\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_pairs_details)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sat_image_path = 'CVACT/'+ self.image_pairs_details[idx][0]\n",
        "        street_image_path = 'CVACT/'+ self.image_pairs_details[idx][1]\n",
        "\n",
        "        # Open and process the satellite image\n",
        "        with Image.open(sat_image_path) as sat_image:\n",
        "            sat_img = sat_transforms_val_student(image= np.asarray(sat_image))['image']\n",
        "        # Open and process the street image\n",
        "        with Image.open(street_image_path) as street_image:\n",
        "            street_img = street_transforms_val_student(image= np.asarray(street_image))['image']\n",
        "        \n",
        "        return sat_img, street_img\n",
        "\n",
        "# Usage\n",
        "test_set = WeightDistillEvalSet(test_image_pairs_details)\n",
        "val_set = WeightDistillEvalSet(val_image_pairs_details)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size= 64, num_workers = 8, shuffle=False, drop_last=False)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size= 64, num_workers = 8, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContrastiveModel(LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # self.teacher_model = TimmModel('convnext_base.fb_in22k_ft_in1k_384', pretrained=True, img_size= 384)\n",
        "        # self.teacher_model.load_state_dict(torch.load(\"pretrained/cvact/convnext_base.fb_in22k_ft_in1k_384/weights_e36_90.8149.pth\", weights_only=True))\n",
        "        # self.teacher_model.eval()\n",
        "        # for param in self.teacher_model.parameters():\n",
        "        #     param.requires_grad = False\n",
        "            \n",
        "        self.sat_model = create_model('faster_vit_0_224', pretrained=True, model_path=\"/tmp/faster_vit_0.pth.tar\")\n",
        "        self.sat_linear = nn.Linear(1000, 1024)\n",
        "        \n",
        "        self.street_model = create_model('faster_vit_0_224', pretrained=True, model_path=\"/tmp/faster_vit_0.pth.tar\")\n",
        "        self.street_linear = nn.Linear(1000, 1024)\n",
        "        \n",
        "        self.loss = nn.CosineEmbeddingLoss()\n",
        "\n",
        "    def forward(self, teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat):\n",
        "        student_sat_features = self.sat_linear(self.sat_model(stud_sat_feat))\n",
        "        student_street_features = self.street_linear(self.street_model(stud_street_feat))\n",
        "        \n",
        "        # with autocast():\n",
        "        #     teacher_sat_features = self.teacher_model(teacher_sat_feat)\n",
        "        #     teacher_street_features = self.teacher_model(teacher_street_feat)\n",
        "        \n",
        "        loss_sat = self.loss(student_sat_features, teacher_sat_features, torch.ones(student_sat_features.shape[0], device= self.device))\n",
        "        loss_street = self.loss(student_street_features, teacher_street_features, torch.ones(student_street_features.shape[0], device= self.device))\n",
        "        \n",
        "        return (loss_sat + loss_street) / 2\n",
        "\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat = batch\n",
        "        loss = self(teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat)\n",
        "\n",
        "        #n = logits.size(0)\n",
        "\t\n",
        "        # -1 for off-diagonals and 1 for diagonals\n",
        "        #labels = 2 * torch.eye(n, device=logits.device) - 1\n",
        "        \n",
        "        # pairwise sigmoid loss\n",
        "        #loss= -torch.sum(F.logsigmoid(labels * logits)) / n\n",
        "    \n",
        "        self.log(\"train_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        \n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat = batch\n",
        "        loss = self(teacher_sat_feat, teacher_street_feat, stud_sat_feat, stud_street_feat)\n",
        "\n",
        "        #n = logits.size(0)\n",
        "\t\n",
        "        # # -1 for off-diagonals and 1 for diagonals\n",
        "        #labels = 2 * torch.eye(n, device=logits.device) - 1\n",
        "        \n",
        "        # # pairwise sigmoid loss\n",
        "        #loss= -torch.sum(F.logsigmoid(labels * logits)) / n\n",
        "\n",
        "        self.log(\"val_loss\", loss.item(), on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "            \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr= 1e-4)\n",
        "        scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs = 1, max_epochs = 512)\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}] #optimizer\n",
        "\n",
        "    def on_save_checkpoint(self, checkpoint):\n",
        "        # Exclude teacher model's weights from checkpoint\n",
        "        checkpoint['state_dict'] = {k: v for k, v in checkpoint['state_dict'].items() if 'teacher_model' not in k}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cvgl/anaconda3/envs/haad/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1702400430266/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ContrastiveModel(\n",
              "  (sat_model): FasterViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Identity()\n",
              "      (conv_down): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (4): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (levels): ModuleList(\n",
              "      (0): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): ConvBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): ConvBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.013)\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.027)\n",
              "          )\n",
              "          (1): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.040)\n",
              "          )\n",
              "          (2): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.053)\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.067)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.067)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (1): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.080)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.080)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (2): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.093)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.093)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (3): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.107)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.107)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (4): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.120)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.120)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (5): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.133)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.133)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "        (global_tokenizer): TokenInitializer(\n",
              "          (pos_embed): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "          (to_global_feature): Sequential(\n",
              "            (pos): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.147)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.160)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.173)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.187)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.200)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "    (head): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  )\n",
              "  (sat_linear): Linear(in_features=1000, out_features=1024, bias=True)\n",
              "  (street_model): FasterViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Identity()\n",
              "      (conv_down): Sequential(\n",
              "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (4): BatchNorm2d(64, eps=0.0001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (5): ReLU()\n",
              "      )\n",
              "    )\n",
              "    (levels): ModuleList(\n",
              "      (0): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): ConvBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): ConvBlock(\n",
              "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.013)\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((64,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.027)\n",
              "          )\n",
              "          (1): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.040)\n",
              "          )\n",
              "          (2): ConvBlock(\n",
              "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (act1): GELU(approximate='none')\n",
              "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (drop_path): DropPath(drop_prob=0.053)\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.067)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.067)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (1): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.080)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.080)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (2): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.093)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.093)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (3): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.107)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.107)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (4): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.120)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.120)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "          (5): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.133)\n",
              "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "            (hat_attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=8, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (hat_mlp): Mlp(\n",
              "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (hat_drop_path): DropPath(drop_prob=0.133)\n",
              "            (hat_pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=256, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (upsampler): Upsample(size=7, mode='nearest')\n",
              "          )\n",
              "        )\n",
              "        (downsample): Downsample(\n",
              "          (norm): LayerNorm2d((256,), eps=1e-06, elementwise_affine=True)\n",
              "          (reduction): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          )\n",
              "        )\n",
              "        (global_tokenizer): TokenInitializer(\n",
              "          (pos_embed): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "          (to_global_feature): Sequential(\n",
              "            (pos): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
              "            (pool): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): FasterViTLayer(\n",
              "        (blocks): ModuleList(\n",
              "          (0): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.147)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.160)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.173)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.187)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): HAT(\n",
              "            (pos_embed): PosEmbMLPSwinv1D(\n",
              "              (cpb_mlp): Sequential(\n",
              "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                (1): ReLU()\n",
              "                (2): Linear(in_features=512, out_features=512, bias=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (attn): WindowAttention(\n",
              "              (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "              (pos_emb_funct): PosEmbMLPSwinv2D(\n",
              "                (cpb_mlp): Sequential(\n",
              "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
              "                  (1): ReLU(inplace=True)\n",
              "                  (2): Linear(in_features=512, out_features=16, bias=False)\n",
              "                )\n",
              "              )\n",
              "            )\n",
              "            (drop_path): DropPath(drop_prob=0.200)\n",
              "            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "            (mlp): Mlp(\n",
              "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "              (act): GELU(approximate='none')\n",
              "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "              (drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "    (head): Linear(in_features=512, out_features=1000, bias=True)\n",
              "  )\n",
              "  (street_linear): Linear(in_features=1000, out_features=1024, bias=True)\n",
              "  (loss): CosineEmbeddingLoss()\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "distill_model = ContrastiveModel.load_from_checkpoint(\"./logs/KD_EMBEDDING_CD_ONLY_CVACT/version_1/checkpoints/tinyvit.ckpt\")\n",
        "distill_model = distill_model.to(device)\n",
        "distill_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model = TimmModel('convnext_base.fb_in22k_ft_in1k_384', pretrained=True, img_size= 384)\n",
        "# model.load_state_dict(torch.load(\"pretrained/cvact/convnext_base.fb_in22k_ft_in1k_384/weights_e36_90.8149.pth\", weights_only=True))\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = model.to(device)\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Scr3JOak-Are",
        "outputId": "e86d566c-f58c-4c86-8115-4f35872c665a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1451/1451 [03:27<00:00,  7.01it/s]\n",
            "100%|██████████| 139/139 [00:20<00:00,  6.62it/s]\n"
          ]
        }
      ],
      "source": [
        "all_street_embed = []\n",
        "all_sat_embed = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_data in tqdm(test_loader):\n",
        "        sat_img, street_img = batch_data\n",
        "        \n",
        "        # Street Image Embeddings\n",
        "        street_features = distill_model.street_linear(distill_model.street_model(street_img.to(device)))\n",
        "        all_street_embed.append(street_features.detach().cpu())\n",
        "\n",
        "        # Original Satellite Image Embeddings\n",
        "        sat_features = distill_model.sat_linear(distill_model.sat_model(sat_img.to(device)))\n",
        "        all_sat_embed.append(sat_features.detach().cpu())\n",
        "        \n",
        "\n",
        "    # Save the embeddings\n",
        "    joblib.dump(torch.vstack(all_street_embed), 'MB1/cvact_test_street_embed.joblib')\n",
        "    joblib.dump(torch.vstack(all_sat_embed), 'MB1/cvact_test_sat_embed.joblib')\n",
        "    \n",
        "\n",
        "    all_street_embed = []\n",
        "    all_sat_embed = []\n",
        "\n",
        "    for batch_data in tqdm(val_loader):\n",
        "        sat_img, street_img = batch_data\n",
        "        \n",
        "        # Street Image Embeddings\n",
        "        street_features = distill_model.street_linear(distill_model.street_model(street_img.to(device)))\n",
        "        all_street_embed.append(street_features.detach().cpu())\n",
        "\n",
        "        # Original Satellite Image Embeddings\n",
        "        sat_features = distill_model.sat_linear(distill_model.sat_model(sat_img.to(device)))\n",
        "        all_sat_embed.append(sat_features.detach().cpu())\n",
        "\n",
        "    # Save the embeddings\n",
        "    joblib.dump(torch.vstack(all_street_embed), 'MB1/cvact_val_street_embed.joblib')\n",
        "    joblib.dump(torch.vstack(all_sat_embed), 'MB1/cvact_val_sat_embed.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_sat_features = joblib.load(open(\"MB1/cvact_val_sat_embed.joblib\", 'rb'))\n",
        "val_street_features = joblib.load(open(\"MB1/cvact_val_street_embed.joblib\", 'rb'))\n",
        "\n",
        "test_sat_features = joblib.load(open(\"MB1/cvact_test_sat_embed.joblib\", 'rb'))\n",
        "test_street_features = joblib.load(open(\"MB1/cvact_test_street_embed.joblib\", 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8883, 1024])\n",
            "torch.Size([8883, 1024])\n",
            "\n",
            "torch.Size([92801, 1024])\n",
            "torch.Size([92801, 1024])\n"
          ]
        }
      ],
      "source": [
        "print(val_sat_features.shape)\n",
        "print(val_street_features.shape)\n",
        "print()\n",
        "print(test_sat_features.shape)\n",
        "print(test_street_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Normalizing features\n",
        "val_street_features = F.normalize(val_street_features, dim=-1)\n",
        "test_street_features = F.normalize(test_street_features, dim=-1)\n",
        "\n",
        "val_sat_features = F.normalize(val_sat_features, dim=-1)\n",
        "test_sat_features = F.normalize(test_sat_features, dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = faiss.StandardGpuResources()\n",
        "\n",
        "val_index = faiss.IndexFlatIP(DIM)\n",
        "val_index = faiss.index_cpu_to_gpu(res, 1, val_index)\n",
        "\n",
        "test_index = faiss.IndexFlatIP(DIM)\n",
        "test_index = faiss.index_cpu_to_gpu(res, 1, test_index)\n",
        "\n",
        "val_index.add(val_sat_features)\n",
        "test_index.add(test_sat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_top_1_per = int(0.01*len(test_street_features))\n",
        "val_top_1_per = int(0.01*len(val_street_features))\n",
        "\n",
        "#Val table searches\n",
        "_, val_ids = val_index.search(x=val_street_features, k=val_top_1_per)\n",
        "\n",
        "#Test table searches\n",
        "_, test_ids = test_index.search(x=test_street_features, k=test_top_1_per)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For CVACT Val, Top 1 Accuracy: 89.98086232128786%\n",
            "For CVACT Val, Top 5 Accuracy: 96.39761341889002%\n",
            "For CVACT Val, Top 10 Accuracy: 97.21940785770573%\n",
            "For CVACT Val, Top 50 Accuracy: 98.58156028368795%\n",
            "For CVACT Val, Top 100 Accuracy: 98.82922436113925%\n",
            "For CVACT Val, Top 88 Accuracy: 98.82922436113925%\n"
          ]
        }
      ],
      "source": [
        "val_matches = {1: 0, 5: 0, 10: 0, 50: 0, 100: 0, val_top_1_per: 0}\n",
        "\n",
        "for i in range(len(val_street_features)):\n",
        "    if i == val_ids[i][0]:\n",
        "        for key in list(val_matches.keys()):\n",
        "            val_matches[key]+=1        \n",
        "    elif i in val_ids[i][0:5]:\n",
        "        for key in list(val_matches.keys())[1:]:\n",
        "            val_matches[key]+=1 \n",
        "    elif i in val_ids[i][0:10]:\n",
        "        for key in list(val_matches.keys())[2:]:\n",
        "            val_matches[key]+=1\n",
        "    elif i in val_ids[i][0:50]:\n",
        "        for key in list(val_matches.keys())[3:]:\n",
        "            val_matches[key]+=1\n",
        "    elif i in val_ids[i][0:100]:\n",
        "        for key in list(val_matches.keys())[4:]:\n",
        "            val_matches[key]+=1\n",
        "    elif i in val_ids[i][0:val_top_1_per]:\n",
        "        for key in list(val_matches.keys())[5:]:\n",
        "            val_matches[key]+=1\n",
        "\n",
        "for keys in val_matches.keys():\n",
        "    val_matches[keys] = (val_matches[keys] * 100) / len(val_street_features)\n",
        "    print(f\"For CVACT Val, Top {keys} Accuracy: {val_matches[keys]}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For CVACT Test, Top 1 Accuracy: 70.55419661426062%\n",
            "For CVACT Test, Top 5 Accuracy: 91.63802114201356%\n",
            "For CVACT Test, Top 10 Accuracy: 93.81687697330848%\n",
            "For CVACT Test, Top 50 Accuracy: 96.66706177735153%\n",
            "For CVACT Test, Top 100 Accuracy: 97.46015667934613%\n",
            "For CVACT Test, Top 928 Accuracy: 98.75755649184815%\n"
          ]
        }
      ],
      "source": [
        "test_matches = {1: 0, 5: 0, 10: 0, 50: 0, 100: 0, test_top_1_per: 0}\n",
        "\n",
        "for i in range(len(test_street_features)):\n",
        "    if i == test_ids[i][0]:\n",
        "        for key in list(test_matches.keys()):\n",
        "            test_matches[key]+=1        \n",
        "    elif i in test_ids[i][0:5]:\n",
        "        for key in list(test_matches.keys())[1:]:\n",
        "            test_matches[key]+=1 \n",
        "    elif i in test_ids[i][0:10]:\n",
        "        for key in list(test_matches.keys())[2:]:\n",
        "            test_matches[key]+=1\n",
        "    elif i in test_ids[i][0:50]:\n",
        "        for key in list(test_matches.keys())[3:]:\n",
        "            test_matches[key]+=1\n",
        "    elif i in test_ids[i][0:100]:\n",
        "        for key in list(test_matches.keys())[4:]:\n",
        "            test_matches[key]+=1\n",
        "    elif i in test_ids[i][0:test_top_1_per]:\n",
        "        for key in list(test_matches.keys())[5:]:\n",
        "            test_matches[key]+=1\n",
        "\n",
        "for keys in test_matches.keys():\n",
        "    test_matches[keys] = (test_matches[keys] * 100) / len(test_street_features)\n",
        "    print(f\"For CVACT Test, Top {keys} Accuracy: {test_matches[keys]}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def calculate_scores(query_features, reference_features, step_size=10000, ranks=[1,5,10]):\n",
        "\n",
        "    topk = copy.deepcopy(ranks)\n",
        "    Q = len(query_features)\n",
        "    R = len(reference_features)\n",
        "    \n",
        "    steps = Q // step_size + 1\n",
        "    \n",
        "    similarity = []\n",
        "    \n",
        "    for i in range(steps):\n",
        "        \n",
        "        start = step_size * i\n",
        "        \n",
        "        end = start + step_size\n",
        "          \n",
        "        sim_tmp = query_features[start:end] @ reference_features.T\n",
        "        \n",
        "        similarity.append(sim_tmp)\n",
        "     \n",
        "    # matrix Q x R\n",
        "    #similarity = torch.cat(similarity, dim=0)\n",
        "    \n",
        "\n",
        "    topk.append(R//100)\n",
        "    \n",
        "    results = np.zeros([len(topk)])\n",
        "    \n",
        "    \n",
        "    bar = tqdm(range(Q))\n",
        "    \n",
        "    for i in bar:\n",
        "        step_idx = i // step_size\n",
        "        local_idx = i % step_size\n",
        "\n",
        "        # Access the correct similarity chunk and row\n",
        "        sim_chunk = similarity[step_idx]\n",
        "        gt_sim = sim_chunk[local_idx, i]  # Ground truth similarity\n",
        "        \n",
        "        # number of references with higher similiarity as gt\n",
        "        higher_sim = sim_chunk[local_idx, :] > gt_sim\n",
        "        \n",
        "         \n",
        "        ranking = higher_sim.sum()\n",
        "        for j, k in enumerate(topk):\n",
        "            if ranking < k:\n",
        "                results[j] += 1.\n",
        "                        \n",
        "        \n",
        "    results = results/ Q * 100.\n",
        " \n",
        "    \n",
        "    bar.close()\n",
        "    \n",
        "    # wait to close pbar\n",
        "    time.sleep(0.1)\n",
        "    \n",
        "    string = []\n",
        "    for i in range(len(topk)-1):\n",
        "        \n",
        "        string.append('Recall@{}: {:.4f}'.format(topk[i], results[i]))\n",
        "        \n",
        "    string.append('Recall@top1: {:.4f}'.format(results[-1]))            \n",
        "        \n",
        "    print(' - '.join(string)) \n",
        "\n",
        "    return results[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8883/8883 [00:00<00:00, 26178.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@1: 90.0371 - Recall@5: 96.3976 - Recall@10: 97.2194 - Recall@top1: 98.8292\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "90.03714961161769"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_scores(val_street_features, val_sat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 92801/92801 [00:07<00:00, 11643.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall@1: 71.0736 - Recall@5: 91.6628 - Recall@10: 93.8255 - Recall@top1: 98.7576\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "71.07358756909947"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "calculate_scores(test_street_features, test_sat_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate(dist_array, top_k):\n",
        "    accuracy = 0.0\n",
        "    data_amount = 0.0\n",
        "    for i in range(dist_array.shape[0]):\n",
        "        gt_dist = dist_array[i,i]\n",
        "        prediction = torch.sum(dist_array[:, i] < gt_dist) \n",
        "        if prediction < top_k:\n",
        "            accuracy += 1.0\n",
        "        data_amount += 1.0\n",
        "    accuracy /= data_amount\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   compute accuracy\n",
            "start\n",
            "top1 : 0.9005966452774964\n",
            "top5 : 0.9636384104469211\n",
            "top10 : 0.9737701227062929\n",
            "top1% : 0.9882922436113926\n"
          ]
        }
      ],
      "source": [
        "print('   compute accuracy')\n",
        "dist_array = 2.0 - 2.0 * torch.matmul(val_street_features, val_sat_features.T)\n",
        "\n",
        "top1_percent = int(dist_array.shape[0] * 0.01) + 1\n",
        "val_accuracy = torch.zeros((1, top1_percent)).cuda()\n",
        "\n",
        "print('start')\n",
        "\n",
        "print('top1', ':', validate(dist_array, 1))\n",
        "print('top5', ':', validate(dist_array, 5))\n",
        "print('top10', ':', validate(dist_array, 10))\n",
        "print('top1%', ':', validate(dist_array, top1_percent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   compute accuracy\n",
            "top1 : 0.7107358756909947\n",
            "top5 : 0.9166280535770088\n",
            "top10 : 0.9382549757006928\n",
            "top1% : 0.9875863406644325\n"
          ]
        }
      ],
      "source": [
        "def validate_batched(street_features, sat_features, top_k, batch_size=1024):\n",
        "    accuracy = 0.0\n",
        "    data_amount = 0.0\n",
        "    num_samples = street_features.shape[0]\n",
        "    \n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        # Get a batch of street features\n",
        "        street_batch = street_features[i:min(i+batch_size, num_samples)]\n",
        "        \n",
        "        # Compute distances for the current batch\n",
        "        dist_batch = 2.0 - 2.0 * torch.matmul(street_batch, sat_features.T)\n",
        "        \n",
        "        # Iterate over the batch\n",
        "        for j in range(dist_batch.shape[0]):\n",
        "            global_index = i + j  # Get the correct global index\n",
        "            \n",
        "            # Ensure global_index is within bounds\n",
        "            if global_index >= num_samples:\n",
        "                break\n",
        "            \n",
        "            # Ground truth distance is the diagonal element in the full matrix\n",
        "            gt_dist = dist_batch[j, global_index]\n",
        "\n",
        "            # Calculate how many distances are smaller than the ground truth distance\n",
        "            prediction = torch.sum(dist_batch[j, :] < gt_dist).item()\n",
        "\n",
        "            if prediction < top_k:\n",
        "                accuracy += 1.0\n",
        "            data_amount += 1.0\n",
        "\n",
        "    accuracy /= data_amount\n",
        "    return accuracy\n",
        "\n",
        "# Assuming test_street_features and test_sat_features are the feature arrays\n",
        "batch_size = 1024\n",
        "top1_percent = int(test_street_features.shape[0] * 0.01) + 1\n",
        "\n",
        "print('   compute accuracy')\n",
        "print('top1', ':', validate_batched(test_street_features, test_sat_features, 1, batch_size))\n",
        "print('top5', ':', validate_batched(test_street_features, test_sat_features, 5, batch_size))\n",
        "print('top10', ':', validate_batched(test_street_features, test_sat_features, 10, batch_size))\n",
        "print('top1%', ':', validate_batched(test_street_features, test_sat_features, top1_percent, batch_size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "haad",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
